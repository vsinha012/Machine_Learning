{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: Convolutional Neural Network \n",
    "#### To train on the mnist dataset. \n",
    "#### Implement Deep CNN\n",
    "#### Optimize CNN\n",
    "\n",
    "\n",
    "#### CNN was designed based on similar structure as Visual Cortex of the brain where image processing happens. \n",
    "#### On earch, millions of years ago, evolutions sped up because creatures developed the sense of vision.  When prey developed the vision, it became cognizant of the surrounding and was able to evade the predator.  Similarly, predator became congnizant of surrounding environments and was able to catch the prey.  Australian zoologist wrote \"Blink of an eye\".\n",
    "\n",
    "#### Yann LeCun, professor of Computer Science at New York University in 1995 introduced the concept of Convolutional Neural Network in 1995. \n",
    "#### \n",
    "\n",
    "### The Core Idea Behind CNN\n",
    "\n",
    "#### 1. Local Connections: Represents how each set of neuron in a cluster is connected to each other, which in turn represents a set of features. \n",
    "#### 2. Layering: Represents the hierarchy in features that are learned. First layer detects the edges, second layer creates smaller part of the features, and third layer creates the faces. \n",
    "#### 3. Spatial Invariance: Represnts the capability of CNN to learn abstractions invariance of size, contrast, rotation, and variation. \n",
    "\n",
    "#### Few Popular CNN\n",
    "#### ImageNet Large Scale Visual Recognition Challenge Winnders\n",
    "#### LeNet, AlexNet, VGGNet, ResNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning an Image\n",
    "#### \n",
    "#### CNN Focuses on smaller and specific patters than the whole image. \n",
    "#### It's convenient and effective to represent a smaller region with fewer parameters, thereby reducing computational complexity.\n",
    "#### \n",
    "#### Input -> Convolutional -> Pooling -> Dense -> Output\n",
    "\n",
    "#### Kernet/Filter element value is multiplied from the top, summed up, and then relu is applied which is max of (computed value, 0). \n",
    "#### Kernel values are derived during the training process or learned by the neural network which are training parameters. (Xavier Distribution)\n",
    "#### Then it moves horizontally to repeat the same computation. \n",
    "#### Once it cannot move horizontally then it moves down vertically to repeat the same computation.\n",
    "#### This is the fundamental idea of convolution operation. \n",
    "#### \n",
    "#### Maxpool (2x2) /Down Sampling\n",
    "#### Another option is Average Pooling\n",
    "#### \n",
    "#### Flatten (converting 2x2 to 4 rows x 1 column ) \n",
    "#### \n",
    "#### Fully Connected/Dense Layer\n",
    "#### \n",
    "#### Output Layer\n",
    "#### \n",
    "#### Convolutional -> Maxpool -> Flatten -> Fully Connected/Dense Layer -> Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n = dimension of image = 6\n",
    "#### f = dimension of filter = 3\n",
    "#### dimension convolution = n-f + 1 = 4\n",
    "#### dimension maxpool = 2\n",
    "#### with padding, n+2p-f+1 = 6\n",
    "#### \n",
    "#### Padding Layer: Padding is nothing but add 0 all around the image then it is called padding. So, a 6x6 image becomes 8x8. \n",
    "#### Essentially valid \n",
    "#### Valid => No padding \n",
    "#### Same => Padding; \n",
    "#### If the image is nxn and then padding is same then the output metrics is same as nxn metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Images\n",
    "#### Image is itself stored in 3x3 dimension\n",
    "#### 1. Red Channel \n",
    "#### 2. Green Channel\n",
    "#### 3. Blue Channel\n",
    "#### \n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3afe7c0290>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled image since it is gray image (one color image) \n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test  = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Shape (60000, 28, 28, 1) Test Shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Training Shape {X_train.shape} Test Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test  = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sequential model and import layer items\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer with 32 filters, 3*3 filter size, stride=1, and no padding\n",
    "model.add(Conv2D(32, \n",
    "                 kernel_size=3, \n",
    "                 activation='relu', \n",
    "                 padding='valid', \n",
    "                 input_shape=(28,28,1)\n",
    "                )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPool2D(pool_size=2)) # MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation = 'relu')) # Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation = 'softmax')) # Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 5408)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                346176    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 347,146\n",
      "Trainable params: 347,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv2D Parameter is 320 because Kernel/Filter size is 3x3 which has 9 elements. 9 elements +1 bias = 10.  10*32 filters = 320\n",
    "#### n-f+1 with no padding, n here is 28, 3 is filter size, so 28-3+1 = 26 for each image, 32 is filter. \n",
    "#### Pooling size is 2, therefore, 26/2 = 13\n",
    "#### 13*13*32 = 5408 \n",
    "#### (5408+1)*63 = 340767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1845 - accuracy: 0.9459 - val_loss: 0.0719 - val_accuracy: 0.9776\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0640 - accuracy: 0.9810 - val_loss: 0.0576 - val_accuracy: 0.9817\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0426 - accuracy: 0.9870 - val_loss: 0.0578 - val_accuracy: 0.9816\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 0.0446 - val_accuracy: 0.9845\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.0473 - val_accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b224771d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          validation_data=(X_test, y_test), \n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.argmax(model.predict(X_train), axis=1)\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[5904    0    4    0    0    0    1    0   10    4]\n",
      " [   0 6695   25    2    0    1    4    7    1    7]\n",
      " [   0    0 5952    0    2    0    0    0    3    1]\n",
      " [   0    0   22 6079    0    1    0    0   23    6]\n",
      " [   0    1    1    0 5782    0    4    1    8   45]\n",
      " [   2    0    2    4    0 5377   11    0   23    2]\n",
      " [   3    0    1    0    2    1 5907    0    4    0]\n",
      " [   1    4   16    3    2    0    0 6199    3   37]\n",
      " [   0    1    1    0    0    0    0    0 5844    5]\n",
      " [   0    0    1    2    1    0    0    1    8 5936]]\n",
      "\n",
      "\n",
      "0.9945833333333334\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_pred=y_train_pred, y_true=y_train))\n",
    "print('\\n')\n",
    "print(accuracy_score(y_pred=y_train_pred, y_true=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 973    0    2    0    0    0    1    0    3    1]\n",
      " [   0 1125    4    0    0    0    3    1    1    1]\n",
      " [   0    0 1021    0    1    0    1    4    4    1]\n",
      " [   0    0    3  998    0    3    0    0    6    0]\n",
      " [   0    0    1    0  962    0    0    0    2   17]\n",
      " [   2    0    1    5    0  880    3    0    1    0]\n",
      " [   4    2    1    0    2    2  945    0    2    0]\n",
      " [   0    3   14    2    0    0    0  994    2   13]\n",
      " [   2    0    2    0    0    1    0    2  963    4]\n",
      " [   1    0    1    1    1    5    0    0    3  997]]\n",
      "\n",
      "\n",
      "0.9858\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_pred=y_test_pred, y_true=y_test))\n",
    "print('\\n')\n",
    "print(accuracy_score(y_pred=y_test_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "##\n",
    "## Transfer Learning\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2 \n",
    "#### Transfer the convolution and pool layer, then add the output layer. \n",
    "#### Transfer the architecture along with the weights and then add output layer or add dense layer and output layer. \n",
    "####\n",
    "#### Advantage: \n",
    "#### 1. These are superior architecture, so transferring the architecture is optimal. \n",
    "#### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
