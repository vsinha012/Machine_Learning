{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language tokenize library\n",
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WELCOME', 'TO', 'THE', 'NLP', 'TRAINING', 'SESSIONS!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'WELCOME TO THE NLP TRAINING SESSIONS!'\n",
    "str1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WELCOME', 'TO', 'THE', 'NLP', 'TRAINING', 'SESSIONS', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize word by word\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WELCOME TO THE NLP TRAINING SESSIONS!', 'YOU WILL BE LEARNING NLP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence by sentence\n",
    "str2 = 'WELCOME TO THE NLP TRAINING SESSIONS! YOU WILL BE LEARNING NLP'\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WELCOME', 'TO'),\n",
       " ('TO', 'THE'),\n",
       " ('THE', 'NLP'),\n",
       " ('NLP', 'TRAINING'),\n",
       " ('TRAINING', 'SESSIONS!')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram means breaking sentence one word by one \n",
    "# Bigrams means breaking sentenc two words at a time. \n",
    "str3 = 'WELCOME TO THE NLP TRAINING SESSIONS!'\n",
    "bg=list(nltk.bigrams(str3.split()))\n",
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WELCOME', 'TO', 'THE'),\n",
       " ('TO', 'THE', 'NLP'),\n",
       " ('THE', 'NLP', 'TRAINING'),\n",
       " ('NLP', 'TRAINING', 'SESSIONS!')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trigrams\n",
    "tg=list(nltk.trigrams(str3.split()))\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WELCOME', 'TO', 'THE', 'NLP'),\n",
       " ('TO', 'THE', 'NLP', 'TRAINING'),\n",
       " ('THE', 'NLP', 'TRAINING', 'SESSIONS!')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ngrams\n",
    "ng = list(nltk.ngrams(str3.split(),4))\n",
    "ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Challenges\n",
    "### 1. Semantic Meaning\n",
    "### 2. Word Sense Disambiguous\n",
    "### 3. Multiple Context\n",
    "### 4. Anaphora Resolution \n",
    "### 5. Exntity Extraction\n",
    "\n",
    "## Context is critical in interpretation \n",
    "### Example: \"Love All\" by itself can be interpreted as love all living beings or humans but if the context is given as Tennis then the meaning changes referring to the score 0 - 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Intents\n",
    "#### Example: Orange can be a fruit, color, or a city in Florida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaphore Resolution: Absence of important entity in text conversation\n",
    "####  When Hrithik made a debut in films, tickets sold out like hot potatoes \n",
    "####  Hrithik and Tickets are two real-world entities but they are made in the same context in such a way that they are interdependent. The second sentence \"Tickets sold out like hot potatoes\" heavily relies on the first sentence.  \n",
    "#### \n",
    "### Semantic Meaning\n",
    "#### There are many good properties available on HDFC Red portal. \n",
    "#### I want to purchase a red carpet from a store. \n",
    "#### Word RED has different meanings in these contexts\n",
    "#### Semantic ambiguity arises when the meaning of a word in the sentence is unclear.  Semantic is all about the meaning and not necessarily the structure of the sentence. \n",
    "#### \n",
    "### Syntactic Ambiguity\n",
    "#### Example: I saw a girl with Telescope can be interpreted as I saw a girl with the help of a telescope or I saw a girl who is carrying telescope. \n",
    "### Understanding Entities\n",
    "#### A 2M solution of CaCl2 consists of 221.82g of Cacl2 dissolved in enough water to make one liter of solution. \n",
    "#### In this example, understanding and extraction of Cacl2 as entity in this context is complex. \n",
    "#### \n",
    "### NLP Challenges: Ambiguity\n",
    "#### 1. Lexical Ambiguity: More than 1 meaning of a word in a sentence. Example: The fisherman went to the bank. \n",
    "#### 2. Syntactic or Grammatical Ambiguity: More than 1 meaning of a sentence.  Ex: Visiting relatives can be boring.\n",
    "#### 3. Referential Ambiguity: Reference of a pronoun.  Ex: The boy told the father about the theft.  He was very upset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "## Data Formats\n",
    "### 1. Structured\n",
    "### 2. Unstructured\n",
    "### 3. Semi-structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "#### 1. Structured Data Format: Data available in a set structure/defined format. \n",
    "#### 2. Unstrcuture Data Format: Data available is an undefined format such as multimedia data like twitter, facebook\n",
    "#### \n",
    "#### Data\n",
    "#### 1. Data at Rest - Static Data, Example: Book\n",
    "#### 2. Data in Motion - Dynamic Data, Example: Social Media like Facebook, Twitter, etc. \n",
    "#### \n",
    "### Unstructured and Semistructured Data Formats\n",
    "#### Text, Image, JSON (Java Server Object Notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "## NLP Pipeline\n",
    "#### Raw Text -> Text Processing -> Feature Extraction -> Modeling \n",
    "#### The last 3 steps are iterative meaning they get repetitive to get better with model process. \n",
    "### 1. Text Processing\n",
    "#### Input information source such as WWW, pdf, word, human speech/voice signal, output is text \n",
    "#### Take input information from sources such as WWW, pdf, word, human speech/voice signal), process it, convert it into a form which is amicable, output is processed text that would allow you to extract the feature. \n",
    "#### \n",
    "## Sequence or Text Processing\n",
    "#### Raw Text -> Noise Entity Removal -> Word Normalization -> Standardization of Text -> Modified Text. \n",
    "#### Noise Entity Removal: Stop words, URLs, Punctuations, Numbers: (1. Convert all the words of document into lower case 2. For efficient processing, we must keep all words in same casing to avoid case sensitivity of text.)\n",
    "#### Word Normalization: Tokenization, Stemming, Lemmatization\n",
    "#### Standardization of Text: Regular Expressions\n",
    "#### Modified Text\n",
    "#### \n",
    "### Word Normalization: Tokenization: \n",
    "#### Break the sentence into separate words. These words are called tokens.  Split words whenever there is a space between them. Treat punctuation marks as separate tokens since punctuation also has meaning. \n",
    "###  Word Normalization: Stemming: \n",
    "#### It takes the root of the words.  It removes the last few words or suffix of a word where it misspelt or incorrect words. \n",
    "### Word Normalization: Lemmatization: \n",
    "#### Get the root of the word.  Example: The word \"Finance\" is the root for Finances, Financial, Financing, Financer, etc. \n",
    "#### \n",
    "#### Wordnet Lemmatization: NLTK: Lexical Database, NLTK is abbreviation of natural languate tool kit. \n",
    "#### SPACY: Parts of Speech\n",
    "#### \n",
    "### NLTK\n",
    "#### Tokenization\n",
    "#### Lemmatization\n",
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore lemmatization using NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying:- studying\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"Studying:-\", lemm.lemmatize(\"studying\")) # Default is treating as noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ninez:- Ninez\n"
     ]
    }
   ],
   "source": [
    "print(\"Ninez:-\", lemm.lemmatize(\"Ninez\")) # Default is treating as noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banking:- Banking\n"
     ]
    }
   ],
   "source": [
    "print(\"Banking:-\", lemm.lemmatize(\"Banking\")) # Default is treating as noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:- Yeses\n"
     ]
    }
   ],
   "source": [
    "print(\"Yes:-\", lemm.lemmatize(\"Yeses\")) # Default is treating as noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boarding'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize(\"Boarding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.stem.porter' has no attribute 'stem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9227eeb65795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"studies:-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"studies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.stem.porter' has no attribute 'stem'"
     ]
    }
   ],
   "source": [
    "print(\"studies:-\", porter.stem(\"studies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LEMMA) running:- run\n"
     ]
    }
   ],
   "source": [
    "print(\"(LEMMA) running:-\", lemma.lemmatize(\"running\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LEMMA) running:- running\n"
     ]
    }
   ],
   "source": [
    "print(\"(LEMMA) running:-\", lemma.lemmatize(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Studying, study, running, run, ran\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:- Studying, study, running, run, ran\n"
     ]
    }
   ],
   "source": [
    "print('String 1:-', lemma.lemmatize(str1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:- Studying, study, running, run, ran\n"
     ]
    }
   ],
   "source": [
    "print('String 1:-', lemma.lemmatize(str1, wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
