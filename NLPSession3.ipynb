{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECAP of DAY2\n",
    "#### Text Processing\n",
    "#### Lexical, Semantic, Syntactic Ambiguity\n",
    "#### Referential\n",
    "#### Mutliple Intents, Anaphora\n",
    "#### Steps of Entity Extraction\n",
    "#### Data Formats\n",
    "#### nltk Introduction\n",
    "#### Lemmatization & Stemming\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Process of extracting meaningful short phrases from sentences by analyzing the parts of speech \n",
    "#### 2. Words or patterns can also be defined.  These should not be a part of chunk and such words are known as chinks\n",
    "#### 3. Chunk pattern are made by normal regular expression which are designed and modified to match the part of speech tags. \n",
    "#### 4. Chinking is a way to remove a chunk from chunk. \n",
    "#### 5. It is simplest technique used for entity detection. \n",
    "#### \n",
    "### Object Standardization\n",
    "#### Some words or symbols which are not present in standard discutionary are also not recognized by any search processes. Examples: hashtags, acronyms, colloquial slangs\n",
    "#### \n",
    "#### Natural Language Inference: Task of determining whether the given hyphothesis logically follows from the premise.  In Layman terms, you need to understand whether the hypothesis is true, while the premise is your only knowledge about the subject. \n",
    "#### \n",
    "### Feature Engineering on Text Data\n",
    "#### Explain N-gram\n",
    "#### Demonstrate the different word embedding modles\n",
    "#### Perform operations on word analogies\n",
    "#### Demonstrate the working of Bag-of-Words\n",
    "#### \n",
    "#### Clean Data -> Feature Extraction -> Model\n",
    "#### Computers do not have any standard representation of words\n",
    "#### Once the text is clearned and normalized, it needs to be transformed into features which can be used for modeling\n",
    "#### Corpus is simply a large text. \n",
    "\n",
    "#### 1. Bag-of-Words\n",
    "#### 2. N-Gram\n",
    "#### 3. Document-Term Matrix\n",
    "#### 4. TF-IDF: Term frequency - Inverse Document Frequency: A handy algorithm that uses the frequency of words to determine how relevant those words are to a given document. It's a relatively simple but intuitive approach to weighting words, allowing it to act as a great jumping off point for a variety of tasks.\n",
    "#### \n",
    "#### 1. N-Grams are combinations of adjacent words or letters of length n in the source text\n",
    "####      a. Spelling error detection\n",
    "####      b. Spelling error correction\n",
    "####   Text comparison\n",
    "####   Information Retreival\n",
    "####   Automatic Text Categorization\n",
    "####   Autocomplete\n",
    "#### 2. Document-Term Matrix\n",
    "#### \n",
    "### One Hot Encoding\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "####  Words can be distributed as vector representation.  We are talking about numerical vector. \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
